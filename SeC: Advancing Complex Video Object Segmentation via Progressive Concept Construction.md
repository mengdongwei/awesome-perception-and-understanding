# SeC：通过渐进式概念构建，推动复杂视频对象分割新范式
《SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction》

**摘要**

视频对象分割（Video Object Segmentation, VOS）是计算机视觉领域的一项核心任务，旨在视频序列中对特定目标进行持续的跟踪与分割。尽管现有技术取得了显著进展，但它们在处理剧烈外观变化、遮挡和复杂场景切换时，仍远未达到人类的水平。其根本原因在于，当前模型严重依赖于表观特征匹配，而忽略了人类所具备的、能够跨越时序动态稳健识别物体的概念理解能力。为了弥补这一差距，本文介绍了一项名为 **SeC (Segment Concept)** 的前沿工作。SeC 框架摒弃了传统的特征匹配范式，转向渐进式地构建和利用高级、以对象为中心的概念表征，从而在复杂场景下实现鲁棒的视频对象分割。

---

### **一、引言：当前视频对象分割的困境**

视频对象分割（VOS）致力于在视频的每一帧中精确地描绘出目标对象的轮廓并进行时序追踪。这项技术是实现高级场景理解的关键，广泛应用于自动驾驶、机器人感知、视频编辑和智能监控等领域。

然而，当前主流的VOS模型，特别是那些基于记忆（memory-based）匹配的方法，虽然在标准测试集上表现优异，但在面对真实世界的复杂挑战时，其性能瓶颈便显现出来。这些方法的核心是通过度量当前帧的物体与先前已观测到的实例之间的相似度来识别目标。这种机制在目标外观变化剧烈（如视角突变、严重遮挡）或场景发生切换时，往往会因为低级视觉线索的不可靠而“跟丢”目标或被外观相似的干扰物误导。

研究者认为，机器与人类在时序感知上的根本差异造成了这一局限。人类的感知并不仅限于表面的相似性，而是通过整合跨帧的观测信息，构建一个关于目标的整体性、概念性的理解。这种高级表征，即“对象级概念”，使人类即使在目标外观发生巨大变化时也能稳健地识别它。例如，在一段视频中，即使场景切换，人类也能凭借“主角正在参加比赛，而非观众”这样的概念认知，准确地持续锁定目标。

基于此洞察，本文的研究者提出，VOS领域需要一次范式转移：**从传统的表观匹配转向概念驱动的分割**。

### **二、SeC：一种概念驱动的分割新框架**

为了实现上述范式转移，研究者们设计了 **SeC (Segment Concept)** 框架。其核心目标是赋予分割模型随时间推移形成并利用高级对象概念的能力，同时在简单场景下保持像素级分割的效率和精度。

#### **2.1 核心思想：从“像素匹配”到“概念构建”**

SeC框架的核心是利用大视觉语言模型（Large Vision-Language Models, LVLMs）强大的视觉理解和世界先验知识来构建和提炼对象级的概念。初步实验表明，即使是当前最先进的SAM 2模型，在包含场景切换的视频中性能也会大幅下降。这暴露了依赖低级视觉相似性的模型在概念推理上的缺失。

相比之下，当研究者向 GPT-4o 这样先进的 LVLM 提供一系列包含外观剧烈变化的参考帧和查询帧时，LVLM 不仅能正确定位目标，还能生成文本解释，论证其推理过程。这证明了LVLM具备超越表层线索、进行概念推理的潜力。

受此启发，SeC的设计旨在将LVLM的概念推理能力隐式地整合到分割流程中，而非依赖明确的文本输入或输出。它将LVLM重新定位为一个**视觉概念提取器**，通过潜在的对象级推理直接指导分割过程。

#### **2.2 方法详解**

SeC模型主要由三大模块构成：LVLM概念指导、场景自适应激活策略，以及像素级关联记忆。

**1. LVLM概念指导 (Concept Guidance)**

为了实现鲁棒的概念级推理，SeC在整个视频处理过程中维护一个稀疏的**关键帧库（keyframe bank）**。

* **构建方式**：该库以第一个标注帧进行初始化，并在后续跟踪中动态更新。当一个新帧与库中已有的关键帧差异显著，且其分割结果置信度较高时，该帧被加入库中。这确保了关键帧的多样性与可靠性。为了平衡效率和语义覆盖范围，库中仅保留初始帧和一个固定大小的、采用FIFO（先进先出）策略管理的近期代表性关键帧缓冲池。
* **概念提取**：受LISA等工作的启发，SeC在关键帧序列的末尾附加一个特殊的 **`<SEG>`** 令牌（token）。当LVLM处理这个包含图像和令牌的序列时，它会将关于目标对象的概念性理解汇总、提炼到该`<SEG>`令牌对应的隐藏状态（hidden state）中。这个隐藏状态向量最终被提取出来，作为**对象级概念指导向量 (object-level concept guidance vector)**。

**2. 场景自适应激活策略 (Scene-Adaptive Activation)**

在视频中，大多数连续帧之间具有高度的时间一致性，此时仅需轻量级的像素匹配便已足够。对每一帧都调用计算成本高昂的LVLM是冗余的。因此，SeC采用了一种场景自适应的激活策略，其灵感来源于人类的适应性感知行为——只在发生显著变化时才进行深度推理。

* **工作机制**：模型首先会检测当前帧与前一帧之间是否存在显著的场景变化。
    * **无变化**：若未检测到场景变化，模型将仅依赖于像素级的关联记忆模块，将经过记忆增强的图像特征直接送入掩码解码器生成最终预测，过程高效。
    * **有变化**：若检测到显著变化，则激活LVLM进行概念推理。生成的概念指导向量会通过一个轻量级的交叉注意力模块（cross-attention）与当前帧的特征进行融合。
* **特征融合**：经过概念增强的空间特征，会与像素级关联记忆模块输出的记忆增强特征进行逐点相加。这种融合方式有效地结合了来自LVLM的高级语义先验和来自记忆模块的低级视觉对应关系，使模型能够在外观剧烈变化时保持稳健。
* **场景变化检测**：该检测通过一个轻量级的、基于HSV颜色空间的检测器实现。具体来说，它计算当前帧与前一帧在色调（Hue）和饱和度（Saturation）通道上的2D颜色直方图，并测量它们之间的**巴氏距离（Bhattacharyya distance）**。当距离超过预设阈值时，即判定为发生场景变化。

**3. 像素级关联记忆 (Pixel-Level Association Memory)**

该模块是SeC进行高效跟踪的基础，它建立在SAM 2的记忆注意力机制之上，并进行了增强。

* **长时记忆增强**：通过扩展时间位置编码，支持更宽的时间窗口（最多22帧）。
* **对象感知过滤**：借鉴SAM2Long的策略，只挑选遮挡评分为零（即物体可见）的帧来构建记忆，确保记忆内容与语义相关，减少无信息帧带来的噪声。

### **三、模型训练：两阶段数据准备与训练策略**

SeC的训练采用了一个精心设计的两阶段方法，分别针对模型的不同功能模块进行优化。

#### **第一阶段：训练像素级关联记忆模块**

此阶段的目标是训练模型进行长期的时序建模。

* **训练数据**：从SA-V训练集中，利用SceneDetect工具筛选出场景转换次数最多的2000个视频。
* **数据处理**：对于每个视频，随机采样24个打乱顺序的帧进行训练。
* **训练细节**：在这一阶段，仅更新记忆注意力模块的参数，模型的其他所有组件（如图像编码器和解码器）保持冻结。模型使用64的批量大小（batch size）和 $5\times10^{-6}$ 的学习率训练40个周期（epoch）。

#### **第二阶段：微调LVLM语义指导模块**

此阶段的目标是训练模型进行概念建模。

* **基础模型**：使用 InternVL 2.5 作为LVLM。
* **训练数据**：从SA-V训练集中选取约19万个对象实例，每个实例至少包含三个可见的掩码（mask）。
* **样本构建**：
    * 为每个训练样本随机选择1到7个参考帧。
    * **独特的标注风格**：为了避免掩码遮挡LVLM感知所需的视觉特征，研究者没有使用传统的alpha混合叠加掩码，而是在目标对象周围绘制**绿色轮廓**来高亮目标。
    * **负采样**：为了增强模型的判别力，样本中还额外包含了0到2个带有不正确标注的**干扰帧（distractor frames）**。
    * 所有图像被统一调整到 $448\times448$ 的分辨率。
* **训练细节**：此阶段采用**LoRA（Low-Rank Adaptation）**对InternVL 2.5进行微调，同时冻结所有SAM 2的参数。模型使用64的批量大小和 $4\times10^{-5}$ 的学习率训练3个周期。所有实验均在8块NVIDIA A800 GPU上完成。

### **四、SeCVOS：为高阶语义理解而生的新基准**

为了更严谨地评估模型在高阶语义推理方面的能力，研究者们还构建了一个全新的视频对象分割基准——**SeCVOS (Semantic Complex Scenarios Video Object Segmentation)**。他们观察到，现有的VOS基准（如DAVIS, YouTube-VOS）已趋于饱和，顶尖模型的性能指标已非常高，这使得在这些基准上的进一步提升难以反映模型鲁棒性的真实进展。更重要的是，现有基准缺乏专门用于评估跨镜头对象推理、长时遮挡等语义挑战的设置。

SeCVOS基准的特点包括：
1.  **高度不连续的帧序列**：包含大量镜头切换。
2.  **跨场景频繁重现**：目标物体会在完全不同的场景中反复出现。
3.  **剧烈的镜头转换和动态相机运动**。

这些特性对依赖局部视觉相似性的传统方法构成了巨大挑战。SeCVOS包含160个经过精心筛选和手动标注的多镜头视频，平均每个视频包含4.26个不同场景，显著超过了现有基准。

### **五、实验结果与分析**

在SeCVOS基准上，SeC的表现远超包括SAM 2及其变体在内的所有先前方法。特别地，随着视频中场景转换次数的增加，SeC的性能优势愈发明显。例如，在包含多个场景变化的视频子集中，SeC的J&F得分比SAM 2.1高出15.1个点，这充分验证了LVLM概念推理对于处理复杂、不连续场景的有效性。

此外，在SA-V、LVOS v2等多个标准VOS基准上，SeC同样取得了SOTA（state-of-the-art）或具有竞争力的性能，证明了其框架的有效性和通用性。

消融实验进一步证实：
* 像素级关联记忆模块在单镜头场景（如SA-V）中能带来显著提升。
* 概念指导模块则在多镜头、语义复杂的SeCVOS上发挥了决定性作用，性能提升了7.8个点。
* 场景自适应激活策略非常高效，仅在不到10%的帧中激活LVLM概念指导，便能实现接近饱和的性能，从而在精度和效率之间取得了极佳的平衡。

### **六、结论**

论文提出的 **SeC** 框架及其配套的 **SeCVOS** 基准，共同为视频对象分割领域的发展指明了一个富有潜力的新方向。通过将大视觉语言模型的概念推理能力引入分割流程，SeC成功地从依赖表观匹配的传统范式，迈向了更接近人类感知的、基于高级概念理解的新范式。实验证明，这种方法能够显著提升模型在处理剧烈外观变化、场景切换等复杂情况下的鲁棒性和准确性。我们有理由相信，这项工作将启发更多关于概念级建模的研究，以实现更长时、更具语义深度的视频理解。