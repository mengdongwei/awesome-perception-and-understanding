# awesome dense grounded understanding

## Resources

| Title                                                                                                                     | Date    | Code                                                           |
| ------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------------------- |
| [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/pdf/2506.24102)              | 2025-06 | [Github](https://github.com/lxtGH/DenseWorld-1M)               |
| [Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding](https://arxiv.org/pdf/2504.10465)                       | 2025-04 | [Github](https://github.com/magic-research/Sa2VA)              |
| [Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos](https://arxiv.org/pdf/2501.04001) | 2025-02 | [Github](https://github.com/magic-research/Sa2VA)              |
| [Aligning and Prompting Everything All at Once for Universal Visual Perception](https://arxiv.org/abs/2312.02153)         | 2023-12 | [Github](https://github.com/shenyunhang/APE)                   |
| [SAM2](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)                            | 2024-07 | [Github](https://github.com/facebookresearch/sam2)             |
| [RAM++: Open-Set Image Tagging with Multi-Grained Text Supervision](https://arxiv.org/abs/2310.15200)                     | 2023-10 | [Github](https://github.com/xinyu1205/recognize-anything)      |
| [RAM: Recognize Anything: A Strong Image Tagging Model](https://arxiv.org/abs/2310.15200)                                 | 2023-06 | [Github](https://github.com/xinyu1205/recognize-anything)      |
| [SAM1](https://ai.meta.com/research/publications/segment-anything/)                                                       | 2023-04 | [Github](https://github.com/facebookresearch/segment-anything) |