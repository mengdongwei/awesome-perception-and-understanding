文章地址： https://arxiv.org/abs/2506.24102
代码/数据地址： https://github.com/lxtGH/DenseWorld-1M

引言

近年来，多模态大型语言模型（MLLM）在场景理解方面取得了显著进展，这在很大程度上得益于大规模、高质量数据集的支撑。然而，现有的视觉语言数据集普遍存在一些局限性，例如，多数数据集缺乏对视觉实体精确的地面位置和相互关系的标注。此外，一些已有的局部化描述数据集（Grounded Caption Datasets）也面临着描述不够详尽、关系缺失以及难以处理高分辨率图像中海量目标等问题。
为了填补这一空白，字节跳动与武汉大学、北京大学的研究者们联合推出了DenseWorld-1M，这是首个面向真实世界场景的大规模、详尽、密集的局部化描述数据集。本文将深入探讨该研究旨在解决的核心问题及其创新的方法论。

研究背景与挑战：现有数据集的局限性

当前最先进的MLLM的卓越性能离不开海量、多样化且高质量的训练数据。然而，随着对机器在真实世界中进行更细粒度理解的需求日益迫-切，现有数据集的不足也愈发凸显。具体而言，主要挑战包括：

缺乏详尽的密集目标描述： 许多数据集仅包含简单的图像级描述，无法提供对图像中每个物体的细致刻画。
缺少局部化信息： 大部分描述数据集不包含物体的位置信息（如边界框或掩码），这限制了模型进行区域级别的理解和推理。
描述简单且上下文缺失： 即便是现有的局部化描述数据集，其描述也往往较为简短，缺少对背景信息、物体间空间关系以及细粒度属性的全面描述。例如，一些数据集的图像分辨率较低，场景相对简单，无法满足对复杂真实世界场景的理解需求。
这些局限性阻碍了MLLM实现从“看懂”到“精通”的跨越，无法像人类一样对视觉信息进行深入解读和交互。


DenseWorld-1M的构建：三阶段自动化标注流程

为了克服上述挑战，研究团队设计了一套创新性的三阶段自动化数据标注流程。该流程旨在利用现有的先进模型，以一种“自下而上”的方式，自动生成海量、高质量的密集局部化描述，完全无需人工参与。

三阶段自动化数据标注流程


第一阶段：像素级标注（Pixel-level Labeling）

此阶段的核心目标是将复杂的场景分解为多个独立的物体，并为每个物体生成精确的分割掩码。

开放词汇的实体分割： 首先，利用如RAM++这样的图像标签模型生成开放词汇的物体标签。
掩码生成与融合： 随后，将这些标签输入到能够进行全景分割的APE模型中，生成初步的物体掩码。同时，研究者们还巧妙地利用SAM（Segment Anything Model）的“分割一切”模式来生成多粒度的掩码，以弥补APE可能存在的遗漏。
掩码后处理与优化： 将APE和SAM生成的掩码集合并，并通过精心设计的合并与细化流程（包括基于IoU的合并策略、非极大值抑制（NMS）等）来消除重复和低质量的掩码，最终得到高质量、无重叠的实体级分割结果。

第二阶段：对象级标注（Object-level Labeling）

在获得精确的物体掩码后，此阶段致力于为每个被分割出的物体生成详尽的描述。

物体裁剪与初步描述： 首先，根据掩码将每个物体从原图中裁剪出来。将裁剪出的单个物体图像输入到强大的MLLM（如InternVL-2.5 78B）中，生成一个简短的、聚焦于物体类别和主要外观特征的初步描述。
融合上下文的详细描述： 仅有裁剪图像会丢失物体与周围环境的关系信息。因此，研究者将带有视觉提示（在原图上高亮显示当前物体）的完整图像和前一步生成的简短文本描述，一同作为提示输入到MLLM中。这使得模型能够生成既包含物体自身细节，又融入了与周围环境关系信息的详尽描述。
验证与过滤： 为了确保描述的准确性，研究团队引入了另一个先进的MLLM（Qwen2.5-VL 72B）作为验证模型，对生成的描述进行一致性检查，并过滤掉不准确的样本。

第三阶段：场景级标注（Scene-level Labeling）

此阶段的目标是将各个物体的详细描述整合，生成一个连贯、流畅且包含空间关系的场景级密集局部化描述。

简单场景直接生成： 对于物体数量较少的简单场景，可以直接将第二阶段生成的所有物体描述和带有视觉提示（如物体ID）的图像输入到MLLM中，生成最终的密集局部化描述。
复杂场景的“分而治之”策略： 对于包含大量物体的复杂场景，模型往往难以处理所有信息。研究者采用了一种“分治”策略：首先将复杂图像分割成多个子图像以降低复杂度，然后为每个子图像生成密集局部化描述，最后再将这些子图像的描述融合成对整个图像的完整描述。

模型创新：加速标注流程的“利器”

尽管上述三阶段流程能够自动化生成高质量数据，但其多次调用大型MLLM模型带来了巨大的计算开销。为了解决这一问题并实现数据集的低成本、可持续扩展，研究者们提出了两个经过微调的“小模型”，在标注流程中扮演了关键角色。

DRC：精细化区域描述模型（Detailed Region Caption Model）


Detailed Region Caption model

DRC模型旨在加速第二阶段的对象级标注过程。它通过创新的方式，使得一个较小的模型（3B参数）能够高效地同时为多个物体生成准确、详细的描述。

特征引用与ID嵌入相结合： 
为了解决在物体密集、复杂的场景中，模型可能难以准确识别被引用的物体的问题，研究者引入了ID嵌入来创建一个更强的空间绑定关系 , 具体实现流程为：
分配ID特殊令牌：首先，为每一个对象随机分配一个ID特殊令牌（ID special token），例如<obj i> 
填充文本嵌入：将这些特殊令牌对应的文本嵌入（text embeddings）填充到其各自的对象掩码（object masks）所覆盖的区域中
通过可学习层处理：将填充后的嵌入（embeddings）通过一个可学习的“ID patch embed”层进行处理。这个层是通过一个二维卷积（2D convolution）实现的，并且卷积核之间没有重叠 。
叠加特征：最后，将经过处理的ID嵌入叠加（superimposing）到视觉补丁嵌入（vision patch embeddings）上
通过这种方式，ID嵌入提供了强大的空间绑定关系，并且在早期就与视觉特征融合，使模型能够更精确地识别被引用的对象 。


SCM：空间描述融合模型（Spatial Caption Merging Model）


Spatial Caption Merging model

SCM模型则用于加速第三阶段的场景级标注。它同样是一个经过微调的小型模型，能够根据给定的图像、物体掩码和各物体的描述，一次性地生成最终的密集局部化场景描述，显著提高了处理复杂场景的效率。

实验与验证：DenseWorld-1M的卓越性能

为了验证DenseWorld-1M数据集和所提出模型的有效性，研究团队进行了一系列广泛的实验。
提升像素级MLLM性能： 在指代表达分割（referring segmentation）和局部化描述生成等任务上，使用DenseWorld-1M进行微调的基线模型（如Sa2VA）在多个指标上均取得了显著提升，尤其是在局部化描述的相关指标（如AP50和Recall）上，证明了该数据集能够有效增强模型的细粒度理解能力。
增强图像级MLLM能力： 无论是对于最先进的开源模型（如Qwen2.5-VL）进行后训练，还是从零开始训练LLaVA-1.5模型，DenseWorld-1M的加入都带来了一致的性能提升，覆盖了包括MMBench、SEEDBench在内的多个主流评测基准。
DRC与SCM的有效性与高效性： 实验证明，DRC模型在区域描述任务上超越了现有的其他模型，而SCM模型在用户研究中也表现出与复杂标注流程相媲美的性能。更重要的是，这两个模型极大地提升了标注效率，例如，在A100 GPU上，DRC能将单个SAM级别图像的对象标注时间从3.2分钟缩短至1.1分钟，而SCM则能将场景级描述的生成时间从2.6分钟锐减至31秒。
与基线模型相比，经过DenseWorld-1M训练的模型能够生成更细粒度和更密集的局部化描述。