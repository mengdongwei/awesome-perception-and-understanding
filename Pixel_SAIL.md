## **Pixel-SAIL：迈向更简洁的像素级理解——单Transformer架构的探索与实践**

### **引言**

近年来，多模态大语言模型 (Multimodal Large Language Models, MLLMs) 在精细化的像素级理解任务中取得了令人瞩目的成就。然而，当前的先进模型普遍依赖于复杂的系统架构，通常集成了如CLIP般的视觉编码器、专用的分割模型以及额外的解码器等多个外部组件。这种高度模块化的设计不仅显著增加了系统的复杂性，也为模型的扩展和优化带来了挑战。

为了应对这一挑战，来自字节跳动和武汉大学的研究者们提出了一个极简而高效的解决方案——**Pixel-SAIL**。该研究探索了一种全新的设计范式，即采用**单一的Transformer模型**来完成需要精确像素定位的理解任务，旨在大幅简化现有像素级MLLM的系统架构。本文将深度解析Pixel-SAIL的核心思想、技术创新及其在多个基准测试中的卓越表现。

### **问题的提出：现有像素级多模态模型的复杂性困境**


当前主流的像素级多模态大语言模型，其架构设计往往遵循着一种“缝合”的思路。如下图(a)和(b)所示，这些模型通常包含多个独立的专家模块：

1. **视觉编码器 (Visual Encoder)**：负责提取图像的初步特征，例如使用CLIP。
2. **大语言模型 (LLM)**：作为核心的理解与推理引擎。
3. **分割专家 (Segmentation Experts)**：用于处理像素级别的分割任务，如集成SAM (Segment Anything Model) 及其解码器。
4. **对象提取模块 (Object Token Extraction Model)**：专门用于从图像中识别并提取对象信息。

这种架构虽然在特定任务上表现出色，但其固有的复杂性带来了诸多问题：

* **系统臃肿**：多个独立组件的集成导致系统庞大且难以维护。
* **性能瓶颈**：模型的最终性能可能受限于任何一个子模块的短板，导致次优解。
* **扩展性受限**：复杂的设计使得模型的进一步扩展和迭代变得异常困难。

正是基于这些痛点，Pixel-SAIL的研究者们开始思考：我们能否摒弃这些额外的“专家”，用一个统一的模型来解决所有问题？

### **Pixel-SAIL的核心思想：回归单一Transformer的极简设计**

Pixel-SAIL的设计哲学是“大道至简”。其灵感来源于近期的**SAIL (Single Transformer as a Unified Vision-Language Model)** 架构，也称为“无编码器 (Encoder-Free)”的MLLM。这类模型摒弃了独立的视觉编码器，而是让单一的Transformer在大型数据集上共同学习视觉和文本的表征。

Pixel-SAIL将这一思想进一步推广到像素级别的理解任务，其核心架构如下图(c)所示，仅包含一个单一的Transformer。该模型直接接收文本、图像和视觉提示作为输入，并统一输出文本和分割掩码。

然而，将SAIL架构直接应用于像素级任务并非一帆风-顺。研究者们发现，一个朴素的基线模型存在以下几个关键缺陷：

1. **特征分辨率低**：由于缺少专门的分割解码器，直接从Transformer的视觉Token重塑的特征图分辨率过低，导致分割掩码质量不佳。
2. **视觉提示理解能力弱**：基线模型直接在低级图像块嵌入上进行池化来获取对象表示，这些嵌入缺乏高级语义信息，使得模型难以准确理解视觉提示的意图。
3. **分割边界粗糙**：由于没有分割专家的“指导”，生成的掩码在物体边缘等细节上表现很差。

### **Pixel-SAIL的三大技术创新**

为了克服上述挑战，研究团队在朴素基线模型的基础上，提出了三项关键的技术改进，共同构成了Pixel-SAIL框架。

#### **1. 可学习的上采样模块 (Learnable Upsampling Module)**

为了解决分割掩码质量差的问题，Pixel-SAIL设计了一个简洁而有效的可学习上采样模块。该模块由多个上采样块构成，每个块包含一个转置2D卷积和一个深度卷积。它的作用是将Transformer输出的低分辨率视觉特征图（例如，从原始图像尺寸的1/16或1/32）高效地提升至更高分辨率（如1/4），从而为生成精细的分割结果提供高质量的特征基础。这一设计在保持架构简洁性的同时，显著提升了模型的像素定位能力。

#### **2. 新颖的视觉提示注入策略 (Visual Prompt Injection)**

针对模型难以理解视觉提示的问题，Pixel-SAIL提出了一种创新的视觉提示注入机制。传统方法通常依赖独立的视觉编码器来提取视觉提示区域的特征，但在无编码器的SAIL架构中这并不可行。

Pixel-SAIL的解决方案是：

  * 在LLM的词汇表中引入一组新的特殊Token，例如`{VP_i}`，专门用于表示不同的视觉提示。
  * 将基于掩码的视觉提示（如点、框）与这些特殊Token的文本嵌入进行关联填充，生成视觉提示Token。
  * 在输入到单一Transformer之前，将这些视觉提示Token与图像块嵌入进行**早期融合**（直接相加）。

通过这种方式，模型可以在文本指令中通过对应的特殊Token（如 “请描述`<VP1>`”）来准确地识别和理解用户指定的视觉区域，极大地增强了对视觉提示的响应能力。

#### **3. 密集的特征蒸馏策略 (Dense Feature Distillation)**

为了弥补因缺乏大规模、高质量分割数据训练而导致的细节分割能力不足，Pixel-SAIL引入了一种特征蒸馏策略。研究者们巧妙地利用了预训练好的分割专家模型（如Mask2Former和SAM2）的知识，但并非将这些模型作为推理时的一部分，而是在训练阶段进行“指导”。

具体来说，Pixel-SAIL利用Mask2Former的像素解码器生成的掩码特征来监督其上采样模块的输出，并利用SAM2的编码器特征来监督其低分辨率的图像特征。这种蒸馏方法可以在不损害模型原有指令跟随能力、且仅带来极小训练开销的情况下，有效地将分割专家的精细化感知能力迁移到单一的Transformer模型中，从而提升最终的分割质量。

### **全新的评测基准：PerBench**

为了更全面地评估像素级MLLM的性能，并推动社区发展，研究团队还构建了一个名为**PerBench** (Pixel-grounded Understanding Benchmark) 的全新评测基准。PerBench具有三大创新和挑战性的特点：

1. **详细的对象描述 (Detailed Object Caption)**：不同于现有基准只包含简短描述，PerBench提供了500个经过人工精校的、包含丰富细节（如外观、属性、用途、与周围物体关系等）的对象描述，用于更深入地评估模型的区域认知和描述能力。
2. **基于视觉提示的多项选择问答 (Visual Prompt-based MCQ)**：为了更公平、定量地评估模型对视觉提示的理解，PerBench设计了500道多项选择题。模型需要根据视觉提示准确感知对象属性，并遵循指令选出正确选项。
3. **视觉-文本联合指代分割 (Visual-Text Referring Segmentation, V-T RES)**：这是一项全新的任务，要求模型同时理解用户输入的视觉提示和文本指令，并分割出被联合指代的对象。这极大地考验了模型的多模态综合理解和像素定位能力。

### **实验结果与分析**

Pixel-SAIL在多个公开的指代分割基准测试中展现了强大的竞争力。

* **性能超越**：在RefCOCO、RefCOCO+和RefCOCOg等数据集上，仅有3B参数量的Pixel-SAIL，其性能全面超越了更大尺寸（7B）且带有视觉专家的模型（如GLaMM、OMG-LLaVA）。
* **架构优势**：这证明了即便没有独立的视觉编码器和分割模型，一个设计精良的单一Transformer架构也能够达到甚至超越更复杂的系统。
* **PerBench表现优异**：在自建的更具挑战性的PerBench上，Pixel-SAIL-3B的综合得分（42.2）也显著高于当前先进的Sa2VA-4B模型（39.0），在三个子任务上均取得了领先。

### **结论与展望**

Pixel-SAIL的提出，为像素级多模态理解领域提供了一个极具吸引力的极简解决方案。它成功地证明了，通过精巧的架构设计和训练策略（可学习上采样、视觉提示注入和特征蒸馏），单一的Transformer模型完全有能力胜任复杂的像素级接地任务，从而摆脱对庞杂外部组件的依赖。

这项工作不仅为构建更简洁、更高效、更易于扩展的MLLM开辟了新的道路，其发布的PerBench基准也将进一步推动社区在精细化对象理解、以及视觉与文本联合推理等前沿方向上的探索。我们有理由相信，这种“少即是多”的设计哲学将在未来的多模态研究中扮演越来越重要的角色。
