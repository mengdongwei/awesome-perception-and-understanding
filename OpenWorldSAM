
## 解决的问题

本研究旨在解决开放词汇分割（open-vocabulary segmentation）中的关键挑战，即让模型能够根据开放式语言提示（如“行人”等简单名词或“穿红衬衫的男人”等复杂描述）对图像中的对象进行精确分割，并处理训练中未曾出现过的类别。传统的图像分割模型通常受限于预定义的词汇集，而像Segment Anything Model (SAM)这样的新兴提示式分割模型虽然能够处理任意对象的分割，但其本身缺乏对语言语义的理解，无法直接通过文本提示进行驱动。

具体而言，该论文致力于解决两个核心挑战：
1.  **语义对齐（Semantic grounding）**：将自由格式的文本描述映射到视觉实体上。
2.  **实例感知（Instance awareness）**：从单个文本描述中识别和区分图像中存在的多个匹配实例，这是现有SAM类架构的一个根本局限性。

本研究的目标是构建一个理想的开放词汇分割器，它应能原生支持文本提示，保留基础模型（如SAM）的知识而不增加巨大的计算开销，并能够分割与单个查询相对应的多个可能实例。

## 使用的方法

本论文提出了一个名为 OpenWorldSAM 的框架，该框架通过一个轻量级的语言适配器，将预训练的Segment Anything Model v2 (SAM2)扩展到开放词汇场景。该方法的核心思想是在保留SAM2强大分割能力的同时，为其注入语言理解能力。

模型主要由以下几个部分组成：

### 冻结的主干网络
使用预训练并冻结的SAM2图像编码器来提取图像特征，同时利用一个多模态视觉-语言编码器（如BEiT-3）来联合编码图像和文本提示，生成融合的语义表示。

### 轻量级适配器
一个仅包含少量可训练参数（约450万个）的语言适配器是本研究的核心。它由以下部分构成：

### MLP投影器
一个两层的多层感知机，用于将多模态编码器输出的高维嵌入投影到SAM2可接受的低维空间。

### Positional tie-breaker与多实例查询生成

**作用**
该组件旨在解决开放词汇分割中的一个关键挑战：当一个语言提示（例如“斑马”）对应于图像中多个实例时，如何让模型能够区分并分割出所有这些实例。传统的类似SAM的架构在处理单个提示时通常只能生成一个掩码，无法处理多实例歧义。位置打断器（Positional tie-breaker）机制的核心作用是为相同的语义查询提供空间上的区分线索，从而使模型能够生成针对不同实例的多个分割掩码。

**实现方法**

1.  **可学习的位置打断器向量**：该方法引入了 $K$ 个可学习的位置打断器向量 $\{t_{1},...,t_{K}\}$。这些向量作为模型的可训练参数，并从正态分布中随机初始化。
2.  **查询扰动**：一个两层MLP投影器首先将多模态编码器输出的语义嵌入 $p_{lang}$ 投影到SAM2可接受的维度，得到语义查询 $u$。随后，这 $K$ 个打断器向量被分别添加到这个语义查询 $u$ 上，生成 $K$ 个不同的、但都带有原始语义信息的查询：$q_{i}=u+t_{i}$，其中 $i=1,...,K$
3.  **训练与学习**：在训练过程中，模型使用匈牙利匹配算法将这 $K$ 个预测掩码与地面真值掩码进行匹配。这种匹配损失自然地促使每个位置打断器 $t_{i}$ 学习专注于不同的空间区域，从而实现对不同实例的定位。值得注意的是，这种机制不需要额外的关于实例数量的监督信息，便能让模型学习到如何将一个语义查询“分解”为多个实例查询。

### 软提示Transformer与掩码解码和类别分配

**作用**
该组件是OpenWorldSAM框架中的语言适配器核心部分。其作用是将带有语义和实例区分信息的查询与SAM2的图像特征进行对齐，以确保最终生成的掩码具有精确的定位。随后，它将精炼后的查询作为提示，送入SAM2的掩码解码器，最终生成像素级的分割掩码并分配类别。

**实现方法**

1.  **软提示Transformer**：该组件是一个包含三层Transformer的模块，用于将语言感知的查询 $\{q_{i}\}$ 与SAM2的图像特征进行交互。它交替执行以下两种注意力机制：
    * **自注意力（Self-Attention）**：允许这 $K$ 个查询之间相互“对话”，以增强它们之间的多样性。
    * **交叉注意力（Cross-Attention）**：使查询能够与SAM2图像编码器生成的第3级图像特征进行交互。这个步骤将语言语义信息与高分辨率的视觉特征对齐，从而实现了语言提示的精确空间定位。
2.  **掩码解码**：经过软提示Transformer精炼后的查询 $\{q_{i}^{\prime}\}$ 被作为提示，送入SAM2的掩码解码器。
3.  **类别分配**：由于掩码的生成过程完全以原始的文本提示 $T$ 为条件，因此最终生成的掩码都会被分配相同的类别标签 $T$。解码器最终会输出 $K$ 个预测掩码以及各自的置信度分数。
4.  **推理**：在推理阶段，可以通过置信度过滤和非极大值抑制（NMS）来去除冗余或低质量的掩码，最终得到用于语义、实例或全景分割的掩码集。
    * **软提示转换器（Soft Prompting Transformer）**：一个三层Transformer模块，通过交替进行自注意力（使查询之间相互对话）和交叉注意力（使查询与图像特征交互），将语言感知的查询与SAM2的高分辨率视觉特征对齐，以实现精确的定位。
3.  **SAM2掩码解码器**：经过适配器处理后的精炼查询作为提示，被送入SAM2的掩码解码器，最终生成K个预测掩码和相应的置信度分数。

### 模型训练数据的准备过程

本研究采取了一种参数高效的训练策略，旨在最大化利用现有预训练模型的知识。

1.  **训练数据集**：模型主要在包含全景分割标注的COCO2017-Stuff数据集上进行训练。训练集包含104k张图像。
2.  **训练策略**：
    * **冻结编码器**：为了保留预训练知识并减少计算开销，SAM2的视觉编码器和多模态BEiT-3编码器在整个训练过程中都被冻结，其权重不进行更新。
    * **仅训练适配器**：唯一可学习的模块是轻量级的语言适配器，包括MLP投影器、位置打断器和软提示转换器。
    * **损失函数与匹配**：在训练过程中，模型对每个训练样本和提示，使用匈牙利匹配算法将K个预测掩码与地面真值掩码进行匹配，然后应用焦点损失（focal loss）来鼓励模型对所有被提示描述的实例进行精确分割。
    * **位置打断器的学习**：位置打断向量被实现为可学习参数，并从正态分布中随机初始化。匈牙利匹配损失自然地促使每个打断向量在训练中专门负责不同的空间区域，而无需关于实例计数的显式监督。
3.  **训练配置**：模型使用AdamW优化器进行训练，学习率为1e-4，批处理大小为8，并在单个NVIDIA A100 GPU上训练了25个周期。图像分辨率对于SAM2设置为1024，对于BEiT-3设置为224。在COCO数据集上，位置打断器的数量设置为20。