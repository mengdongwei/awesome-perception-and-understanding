## **摘要**
继 Segment Anything Model (SAM) 在图像分割领域取得里程碑式成功后，Meta FAIR 实验室再次推出了其重量级续作——SAM 2。这不仅仅是一次简单的升级，更是将“分割一切”的理念从静态图像拓展至动态视频领域的奠基性工作。SAM 2 作为一个统一了图像与视频分割的基础模型，通过引入全新的任务范式、创新的模型架构和迄今为止规模最大的视频分割数据集，显著提升了分割的精度、速度和交互体验。本文将深入剖析 SAM 2 背后的三大核心支柱：可提示化视觉分割（PVS）任务、引入流式记忆的Transformer模型架构，以及高效的数据引擎与SA-V数据集。

## **一、 背景：从图像到视频，通用分割面临的挑战**
尽管初代 SAM 能够在图像中精确地分割出任意对象，但现实世界是动态的。视频作为承载时空信息的媒介，其分割任务远比静态图像复杂。将分割能力从图像域无缝迁移到视频域，面临着以下核心挑战：
1. **时空复杂性：** 视频中的物体会经历复杂的运动、形变、光照变化，甚至被完全遮挡后再次出现，这要求模型必须具备强大的时空理解与追踪能力。
2. **数据质量与效率：** 视频数据往往伴随着运动模糊、分辨率较低等问题。同时，视频包含海量帧，如何实现高效、实时的处理是关键的工程挑战。
**现有方法与数据集的局限：** 传统的视频对象分割（VOS）方法和数据集，往往局限于特定的物体类别（如人、车），且通常只支持在第一帧进行标注，缺乏交互式修正错误的能力。现有数据集的规模和多样性，远不足以训练出一个能够“分割视频中万物”的通用模型。

为了系统性地解决这些问题，SAM 2 团队从任务定义、模型设计和数据构建三个层面进行了全面的创新。

### **二、 核心方法：任务、模型与数据的三位一体**
SAM 2 的成功并非源于单一技术的突破，而是一个由任务、模型和数据构成的协同创新体系。

#### **统一任务：** 可提示化视觉分割 (Promptable Visual Segmentation, PVS)
PVS 任务是 SAM2 的核心理念，它将图像领域的交互式分割思想推广到了视频领域，构建了一个更通用、更灵活的人机交互范式。其关键特性包括：
1. **任意帧提示：** 用户可以在视频的任意一帧上，通过点、框或掩码（mask）等方式给出提示，来指定或修正感兴趣的分割对象。
即时响应与全局传播：模型在接收到提示后，不仅会立刻在该帧上生成分割结果以提供即时反馈，还会自动将该信息传播到整个视频，生成完整的时空掩码（即“masklet”）。
2. **迭代式修正：** 如果模型在某些帧上“跟丢”了目标或分割不准，用户可以在新的帧上提供额外提示，模型能够利用这些新信息对整个视频的分割结果进行迭代优化。这种设计极大地提升了视频标注的效率和精度，使得用户仅需少量交互就能获得高质量的视频分割结果，摆脱了传统VOS方法对首帧高质量掩码的严重依赖。

#### **模型架构：** 
引入流式记忆的Transformer。为了实现 PVS 任务，SAM 2 设计了一种新颖的流式处理架构，可以看作是初代 SAM 向视频领域的自然延伸。其核心在于引入了记忆机制。模型主要由以下几个模块构成：

1. **图像编码器 (Image Encoder)：** 采用高效的分层视觉 Transformer（Hiera），对视频的每一帧进行一次性编码，提取出多尺度的视觉特征。
   
2. **记忆注意力模块 (Memory Attention)：** 这是实现跨帧信息融合的关键。它是一个 Transformer 模块，能够让当前帧的特征与记忆库中存储的过去帧的特征进行交叉注意力计算。通过这种方式，模型在处理当前帧时，能够“回顾”目标对象在过去的样子和位置，从而更好地处理遮挡、形变等挑战。
   
3. **记忆编码器 (Memory Encoder)：** 负责将当前帧的预测结果和图像编码器的特征进行融合，生成新的记忆，并存入记忆库中。
   
4. **提示编码器与掩码解码器 (Prompt Encoder & Mask Decoder)：** 基本沿用了初代 SAM 的设计，负责编码用户的提示（点、框、掩码）并将其与经过记忆模块增强后的帧特征结合，最终预测出分割掩码。一个重要的创新是，解码器增加了一个遮挡预测头，用于判断目标对象在当前帧是否可见，从而能够处理对象完全消失的情况。
   
5. **记忆库 (Memory Bank)：** 这是 SAM 2 的“长期记忆”与“短期记忆”中心。它以先进先出（FIFO）队列的形式，存储着两种关键信息：包含用户提示的历史帧 和 近期处理过的历史帧。  这些记忆不仅有空间特征图，还包括从掩码解码器输出中提取的、代表对象高级语义的轻量级“对象指针”。并将时间位置信息嵌入到近期处理过的历史帧中，允许模型表示短期物体运动，但不包含到提示帧的记忆中，因为提示帧更稀疏且时间步也不一致 。当处理单张图像时，记忆库为空，该模型就退化为与初代 SAM 类似的工作模式，从而实现了图像和视频任务的统一。

6. **目标指针Object pointers:** Object pointers 是一种轻量级的向量，旨在捕捉被分割对象的高级语义信息。
    - **来源：** 它直接来自于模型中掩码解码器（mask decoder）的输出。具体来说，模型将与输出掩码相对应的“掩码token”（mask token）作为该帧的 object pointer。
    - **存储：** 这些 object pointers 作为一个列表存储在记忆库（memory bank）中，与存储空间特征的记忆分开。
    - **使用：** 在进行分割时，模型的记忆注意力（memory attention）模块会同时对空间记忆特征和这些 object pointers 进行交叉注意力计算。
    - **维度：** 在具体实现中，一个256维的 object pointer 会被分割成4个64维的token，以便于和记忆库进行交叉注意力计算。
    - **作用与效果：** 根据论文中的消融实验（Ablation studies）结果，object pointers 的作用体现在以下几个方面：它能显著提升模型在 SA-V验证集 以及具有挑战性的 LVOSv2（长视频分割基准）上的性能 。然而，在对9个零样本（zero-shot）视频数据集的平均性能上，它并没有带来提升 。
    - **结论：** 尽管 object pointers 未能提升在所有数据集上的平均表现，但鉴于它在关键的、更具挑战性的数据集（如长视频分割和更泛化的 SA-V 数据集）上带来了显著的性能增益，研究人员最终决定默认启用这一设计。


#### 数据引擎与SA-V数据集
高质量、大规模、高多样性的数据是训练强大基础模型的基石。为此，SAM2 团队构建了一个强大的数据引擎 (Data Engine) 和一个全新的视频分割数据集 SA-V。
1. **数据引擎：** 这是一个“人机协同”的数据闭环系统。标注员使用 SAM 2 模型进行辅助标注，当遇到模型处理不好的“硬性案例”时，标注员会进行人工修正。这些修正后的高质量数据又被反哺给模型，用于下一轮的训练和迭代。这个过程分为三个阶段：
 
   - 阶段一 (SAM per frame)：完全手动，逐帧使用初代 SAM 进行标注，速度慢但质量高，主要用于构建高质量的验证集和测试集。
   - 阶段二 (SAM + SAM2-Mask)：引入 SAM2 进行掩码传播（此阶段SAM 2 Mask只接受mask作为prompt，还没有利用memory）。标注员在关键帧上制作掩码，由模型追踪分割后续帧（标注员可以修改SAM2-Mask 预测的mask，模型在修改的基础上重新预测后续帧）。使用该阶段制作的数据训练更新SAM2-mask2次。最终效率提升约5.1倍。
   - 阶段三 (Full SAM 2)：采用具备完整交互能力的 SAM 2，标注员只需通过简单的点击等操作即可修正模型的预测，整个流程效率比阶段一提升了8.4倍。使用该阶段制作的数据训练更新SAM2 5次。

2. **质量验证：** 为了保证高标准的标注质量，论文引入了一个验证步骤 ：安排另一组独立的标注员，负责将每个已标注的 masklet (时空掩码) 的质量验证为“合格”或“不合格” 。“合格” 指的是能够在所有帧中正确且一致地跟踪目标对象 。“不合格” 指的是目标对象有明确的边界，但 masklet 的跟踪不正确或不一致。不合格的 masklet 会被退回至标注流程进行修正 。任何没有明确定义对象的 masklet 都将被完全舍弃。

3. **自动化 masklet 生成：** 确保标注的多样性对于实现“分割一切”能力至关重要。由于人类标注员通常更倾向于关注显著物体，论文通过自动生成的 masklet来扩充标注。此举有两个目的：
   - 一是增加标注的覆盖范围，
   - 二是帮助识别模型的失败案例。
  
为了生成自动 masklet，论文会在视频的第一帧上用一个规则的点网格来提示 SAM 2，从而生成候选的 masklet。
这些候选 masklet 随后会被送到验证步骤进行筛选。被标记为“合格”的自动化 masklet 会被添加到 SA-V 数据集中。而被识别为“不合格”的 masklet（即模型的失败案例）则会被抽样，并提供给标注员，在数据引擎的第三阶段中使用 SAM 2 进行循环修正。
这些自动生成的 masklet 不仅覆盖了大型、显著的中心物体，也覆盖了背景中各种大小和位置的物体。

4. **SA-V (Segment Anything Video) 数据集：** 通过数据引擎，团队构建了迄今为止规模最大的视频分割数据集。
    - 规模：包含 5.09万 个视频，64.26万个时空掩码（masklet），总计 3550万 个掩码实例，其掩码数量是任何现有VOS数据集的53倍以上。
    - 多样性：视频内容覆盖室内外各种场景，由来自47个国家的用户拍摄，具有良好的地理多样性。标注内容不仅包括完整的物体，还大量包含了物体的部件和子部件，真正践行“分割万物”的理念。
    - 挑战性：数据集中包含了大量小物体、遮挡、快速运动等复杂场景，其“消失后重现”的比例（Disappearance Rate）高达42.5%，极具挑战性。
  
### 三、 实验效果：全面超越，树立新标杆
SAM 2 在广泛的零样本（zero-shot）评测中展现了其卓越的性能。
1. **交互式视频分割：** 在模拟真实用户交互的场景下，SAM 2 仅需 1/3 的交互次数 就能达到甚至超越当前最先进（SOTA）的 VOS 模型（如 XMem++、Cutie）结合初代 SAM 的性能。
2. **标准视频对象分割 (VOS)：** 在传统的、仅使用首帧掩码进行分割的VOS任务上，SAM 2 的表现同样全面优于为该任务专门设计的各类SOTA模型。
3. **图像分割：** 令人惊喜的是，SAM 2 不仅是更强的视频分割模型，也是一个更强的图像分割模型。在与初代 SAM 相同的图像分割基准测试中，SAM 2 的精度更高，同时速度快了6倍，这主要得益于其采用了更高效的 Hiera 图像编码器。
4. **SA-V 基准测试：** 在全新的、更具挑战性的 SA-V 测试集上，SAM 2 的性能远超其他所有现有方法，充分证明了其在开放世界、分割“任何物体”方面的领先能力。

### 四、 总结与展望
SAM 2 通过其创新的PVS任务定义、带记忆的流式模型架构和大规模的SA-V数据集，成功地将“分割一切”的能力从静态图像推广到了复杂的视频领域。它不仅在视频分割任务上树立了新的SOTA标准，还反哺了图像分割，实现了更高的精度和效率。

SAM 2 在静态图像和视频领域都表现出色，但在某些特定场景下仍会遇到困难。**当前挑战：**
-  跨镜头和复杂场景下的目标分割： 该模型可能无法在镜头切换时分割目标，或者在拥挤场景、长时间遮挡后以及长视频中丢失或混淆目标。
- 细微细节和快速移动目标的跟踪： SAM 2 在精确跟踪具有非常薄或精细细节的目标时表现不佳，尤其是在这些目标快速移动时。
- 外观相似的相邻目标： 当存在外观相似的相邻目标时（例如，多个相同的杂耍球），SAM 2 也会面临挑战。

**针对上述挑战的改进措施：** 为了缓解这些问题，论文设计了在任意帧中提示 SAM 2 的功能：如果模型丢失目标或出现错误，通过在额外帧上进行细化点击，可以在大多数情况下快速恢复正确的预测。此外，将更明确的运动建模整合到 SAM 2 中，可以减轻在处理细微细节、快速移动目标以及外观相似目标时的错误。

**多目标跟踪与效率提升：** 虽然 SAM 2 可以同时跟踪视频中的多个对象，但它目前是独立处理每个对象，仅利用共享的逐帧嵌入，而没有对象间的通信。尽管这种方法简单，但整合共享的对象级上下文信息有助于提高效率。

**数据引擎的未来发展：** 目前的数据引擎依赖人工标注者来验证 masklet 质量并选择需要校正的帧。未来的发展可以考虑自动化这一过程，以提高效率。